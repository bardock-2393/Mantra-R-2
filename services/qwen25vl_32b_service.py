"""
Qwen2.5-VL-32B Service Module
Handles Qwen2.5-VL-32B-Instruct local inference and GPU optimization
Based on official Hugging Face documentation: https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct
"""

import os
import time
import torch
import numpy as np
from PIL import Image
from typing import Dict, List, Optional, Tuple
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor

# Import analysis templates from the old project
try:
    from analysis_templates import generate_analysis_prompt
    ANALYSIS_TEMPLATES_AVAILABLE = True
    print("‚úÖ Analysis templates imported successfully")
except ImportError:
    ANALYSIS_TEMPLATES_AVAILABLE = False
    print("‚ö†Ô∏è Analysis templates not available, using fallback prompts")

# Import qwen-vl-utils with proper error handling
try:
    from qwen_vl_utils import process_vision_info
    QWEN_VL_UTILS_AVAILABLE = True
    print("‚úÖ qwen-vl-utils imported successfully")
except ImportError:
    QWEN_VL_UTILS_AVAILABLE = False
    print("‚ö†Ô∏è qwen-vl-utils not available, using fallback implementation")
    
    def process_vision_info(messages):
        """Fallback implementation for process_vision_info"""
        image_inputs = []
        video_inputs = []
        
        try:
            if not messages or not isinstance(messages, list):
                print("‚ö†Ô∏è Invalid messages format")
                return image_inputs, video_inputs
                
            for message in messages:
                if not isinstance(message, dict) or "content" not in message:
                    continue
                    
                content_list = message["content"]
                if not isinstance(content_list, list):
                    continue
                    
                for content in content_list:
                    if not isinstance(content, dict):
                        continue
                        
                    content_type = content.get("type")
                    if content_type == "image":
                        # Handle image processing
                        print("üñºÔ∏è Processing image content...")
                        # For now, just skip images since we're focusing on video
                        pass
                    elif content_type == "video":
                        # Handle video processing
                        video_path = content.get("video", "")
                        print(f"üé¨ Processing video content: {video_path}")
                        
                        # Verify video file exists and is accessible
                        if video_path and os.path.exists(str(video_path)):
                            # Check if it's a valid video file
                            try:
                                file_size = os.path.getsize(str(video_path))
                                if file_size > 0:
                                    video_inputs.append(video_path)
                                    print(f"‚úÖ Video file found and added: {video_path} ({file_size} bytes)")
                                else:
                                    print(f"‚ö†Ô∏è Video file is empty: {video_path}")
                            except OSError as e:
                                print(f"‚ö†Ô∏è Error accessing video file {video_path}: {e}")
                        else:
                            print(f"‚ö†Ô∏è Video file not found: {video_path}")
            
            print(f"üìä Processed: {len(image_inputs)} images, {len(video_inputs)} videos")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error in fallback process_vision_info: {e}")
            import traceback
            traceback.print_exc()
            # Return empty lists on error
            image_inputs = []
            video_inputs = []
        
        return image_inputs, video_inputs

from config import Config
from services.gpu_service import GPUService
from services.performance_service import PerformanceMonitor

class Qwen25VL32BService:
    """Local GPU-powered Qwen2.5-VL-32B-Instruct service for video analysis"""
    
    def __init__(self):
        self.device = torch.device(Config.GPU_CONFIG['device'] if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.gpu_service = GPUService()
        self.performance_monitor = PerformanceMonitor()
        self.is_initialized = False
        
    async def initialize(self):
        """Initialize the Qwen2.5-VL-32B-Instruct model on GPU"""
        try:
            print(f"üöÄ Initializing Qwen2.5-VL-32B-Instruct on {self.device}...")
            
            # Check GPU availability
            if not torch.cuda.is_available():
                raise RuntimeError("CUDA not available. GPU is required for Qwen2.5-VL-32B.")
            
            # Check model path
            print(f"üîç Model path: {Config.QWEN25VL_32B_MODEL_PATH}")
            if not Config.QWEN25VL_32B_MODEL_PATH:
                raise RuntimeError("QWEN25VL_32B_MODEL_PATH is empty or None")
            
            # Initialize GPU service
            await self.gpu_service.initialize()
            
            # Load processor (handles both text and image/video inputs)
            print(f"üìù Loading processor from {Config.QWEN25VL_32B_MODEL_PATH}...")
            try:
                # Set min_pixels and max_pixels for optimal performance
                min_pixels = Config.QWEN25VL_32B_CONFIG['min_pixels']
                max_pixels = Config.QWEN25VL_32B_CONFIG['max_pixels']
                
                # Get HF token for authentication
                hf_token = Config.QWEN25VL_32B_CONFIG.get('hf_token', '')
                if not hf_token:
                    # Try to get token directly from environment as fallback
                    hf_token = os.getenv('HF_TOKEN', '')
                    print(f"‚ö†Ô∏è No HF_TOKEN found in config, trying environment variable: {'Found' if hf_token else 'Not found'}")
                
                if hf_token and len(hf_token) > 0:
                    print(f"üîë Using HF token: {hf_token[:10]}...{hf_token[-4:] if len(hf_token) > 14 else ''}")
                else:
                    print("‚ö†Ô∏è No HF_TOKEN available, trying without authentication")
                
                # Try loading with token first
                try:
                    self.processor = AutoProcessor.from_pretrained(
                        Config.QWEN25VL_32B_MODEL_PATH,
                        min_pixels=min_pixels,
                        max_pixels=max_pixels,
                        trust_remote_code=True,
                        token=hf_token  # Add token for authentication
                    )
                except Exception as token_error:
                    print(f"‚ö†Ô∏è Loading with token failed: {token_error}")
                    if hf_token:
                        print("üîÑ Trying without token...")
                        # Try again without token
                        self.processor = AutoProcessor.from_pretrained(
                            Config.QWEN25VL_32B_MODEL_PATH,
                            min_pixels=min_pixels,
                            max_pixels=max_pixels,
                            trust_remote_code=True
                        )
                    else:
                        raise token_error
                
                # Verify processor loaded successfully
                if self.processor is None:
                    raise RuntimeError("Processor failed to load - returned None")
                print(f"‚úÖ Processor loaded successfully: {type(self.processor).__name__}")
                
            except Exception as e:
                print(f"‚ùå Processor loading failed: {e}")
                print(f"   Model path: {Config.QWEN25VL_32B_MODEL_PATH}")
                print(f"   Error type: {type(e).__name__}")
                
                # Try alternative model path if available
                alternative_path = "Qwen/Qwen2.5-VL-32B-Instruct"
                if Config.QWEN25VL_32B_MODEL_PATH != alternative_path:
                    print(f"üîÑ Trying alternative model path: {alternative_path}")
                    try:
                        self.processor = AutoProcessor.from_pretrained(
                            alternative_path,
                            min_pixels=min_pixels,
                            max_pixels=max_pixels,
                            trust_remote_code=True,
                            token=hf_token
                        )
                        print(f"‚úÖ Processor loaded successfully from alternative path")
                    except Exception as alt_error:
                        print(f"‚ùå Alternative path also failed: {alt_error}")
                        raise RuntimeError(f"Failed to load processor from both paths: {e}")
                else:
                    raise RuntimeError(f"Failed to load processor: {e}")
            
            # Load tokenizer as fallback
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    Config.QWEN25VL_32B_MODEL_PATH,
                    trust_remote_code=True,
                    token=hf_token  # Add token for authentication
                )
                print(f"‚úÖ Tokenizer loaded successfully: {type(self.tokenizer).__name__}")
            except Exception as e:
                print(f"‚ö†Ô∏è Tokenizer loading failed, using processor only: {e}")
                self.tokenizer = None
            
            # Load model with optimizations for 32B model
            print(f"ü§ñ Loading Qwen2.5-VL-32B-Instruct model from {Config.QWEN25VL_32B_MODEL_PATH}...")
            try:
                # Use SDPA for better compatibility and performance
                self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    Config.QWEN25VL_32B_MODEL_PATH,
                    torch_dtype=torch.bfloat16,  # Use bfloat16 for 32B model
                    attn_implementation="sdpa",  # Use SDPA instead of flash attention
                    device_map="auto",  # Let accelerate handle device mapping
                    trust_remote_code=True,
                    token=hf_token  # Add token for authentication
                )
                
                # Verify model loaded successfully
                if self.model is None:
                    raise RuntimeError("Model failed to load - returned None")
                print(f"‚úÖ Qwen2.5-VL-32B-Instruct model loaded successfully: {type(self.model).__name__}")
                
            except Exception as e:
                print(f"‚ùå Model loading failed: {e}")
                print(f"   Model path: {Config.QWEN25VL_32B_MODEL_PATH}")
                print(f"   Error type: {type(e).__name__}")
                
                # Try alternative model path if available
                alternative_path = "Qwen/Qwen2.5-VL-32B-Instruct"
                if Config.QWEN25VL_32B_MODEL_PATH != alternative_path:
                    print(f"üîÑ Trying alternative model path: {alternative_path}")
                    try:
                        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                            alternative_path,
                            torch_dtype=torch.bfloat16,
                            attn_implementation="sdpa",
                            device_map="auto",
                            trust_remote_code=True,
                            token=hf_token
                        )
                        print(f"‚úÖ Model loaded successfully from alternative path")
                    except Exception as alt_error:
                        print(f"‚ùå Alternative path also failed: {alt_error}")
                        raise RuntimeError(f"Failed to load model from both paths: {e}")
                else:
                    raise RuntimeError(f"Failed to load model: {e}")
            
            # Don't manually move to device when using device_map="auto"
            # The model is already properly placed by accelerate
            print(f"üöÄ Model device mapping handled by accelerate")
            
            # Enable torch.compile for massive speed improvement
            try:
                self.model = torch.compile(self.model, mode='reduce-overhead')
                print("‚úÖ Torch compile enabled - expect 2-3x speed improvement")
            except Exception as e:
                print(f"‚ö†Ô∏è Torch compile failed: {e}")
            
            # Warm up the model
            await self._warmup_model()
            
            self.is_initialized = True
            print(f"‚úÖ Qwen2.5-VL-32B-Instruct initialized successfully on {self.device}")
            
        except Exception as e:
            print(f"‚ùå Failed to initialize Qwen2.5-VL-32B-Instruct: {e}")
            raise
    
    async def _warmup_model(self):
        """Warm up the Qwen2.5-VL-32B model for optimal performance"""
        try:
            print("üî• Warming up Qwen2.5-VL-32B model...")
            
            # Create a simple warmup prompt
            warmup_messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Hello, how are you?"}
                    ]
                }
            ]
            
            # Process text-only warmup
            text = self.processor.apply_chat_template(
                warmup_messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt"
            )
            
            # Move inputs to the same device as the model (let accelerate handle this)
            # Get the device from the model's first parameter
            model_device = next(self.model.parameters()).device
            inputs = inputs.to(model_device)
            
            # Generate warmup response
            with torch.no_grad():
                _ = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False
                )
            
            print("‚úÖ Qwen2.5-VL-32B model warmed up successfully")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Model warmup failed: {e}")
    
    async def analyze(self, video_path: str, analysis_type: str, user_focus: str) -> str:
        """Analyze video using Qwen2.5-VL-32B-Instruct - main analysis method"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Qwen2.5-VL-32B service not initialized")
            
            print(f"üé¨ Starting video analysis: {video_path}")
            print(f"   Analysis type: {analysis_type}")
            print(f"   User focus: {user_focus}")
            
            # Use the existing analyze_video method
            result = await self.analyze_video(video_path, analysis_type, user_focus)
            
            print(f"‚úÖ Video analysis completed successfully")
            return result
            
        except Exception as e:
            print(f"‚ùå Video analysis failed: {e}")
            # Return a fallback response
            return f"Video analysis failed. Error: {str(e)}. Please try again or contact support."
    
    async def analyze_video(self, video_path: str, analysis_type: str, user_focus: str) -> str:
        """Analyze video using local GPU-powered Qwen2.5-VL-32B-Instruct"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Qwen2.5-VL-32B service not initialized")
            
            print(f"üé¨ Analyzing video with Qwen2.5-VL-32B: {video_path}")
            
            # Debug: Check file path details
            print(f"üîç Video path details:")
            print(f"   Original path: {video_path}")
            print(f"   Absolute path: {os.path.abspath(video_path)}")
            print(f"   File exists: {os.path.exists(video_path)}")
            print(f"   Is file: {os.path.isfile(video_path)}")
            print(f"   File size: {os.path.getsize(video_path) if os.path.exists(video_path) else 'N/A'} bytes")
            
            # Check if the path is relative and convert to absolute
            if not os.path.isabs(video_path):
                # Try to resolve relative path
                current_dir = os.getcwd()
                abs_path = os.path.join(current_dir, video_path)
                print(f"   Resolved relative path: {abs_path}")
                print(f"   Resolved path exists: {os.path.exists(abs_path)}")
                
                if os.path.exists(abs_path):
                    video_path = abs_path
                    print(f"‚úÖ Using resolved absolute path: {video_path}")
                else:
                    print(f"‚ö†Ô∏è Resolved path also doesn't exist: {abs_path}")
            
            # Generate analysis prompt using the same system as the old project
            if ANALYSIS_TEMPLATES_AVAILABLE:
                analysis_prompt = generate_analysis_prompt(analysis_type, user_focus)
                print("‚úÖ Using analysis templates from old project")
            else:
                analysis_prompt = self._generate_fallback_analysis_prompt(analysis_type, user_focus)
                print("‚ö†Ô∏è Using fallback analysis prompt")
            
            # Generate analysis using Qwen2.5-VL-32B
            analysis_result = await self._generate_analysis(analysis_prompt, video_path)
            
            return analysis_result
            
        except Exception as e:
            print(f"‚ùå Video analysis failed: {e}")
            raise RuntimeError(f"Video analysis failed: {e}")
    
    def _generate_fallback_analysis_prompt(self, analysis_type: str, user_focus: str) -> str:
        """Generate fallback analysis prompt when templates are not available"""
        base_prompts = {
            'general': "Analyze this video and provide a comprehensive overview.",
            'behavioral': "Analyze the behavior patterns and actions in this video.",
            'technical': "Provide a technical analysis of this video content.",
            'narrative': "Analyze the narrative structure and storytelling elements.",
            'forensic': "Conduct a forensic analysis of this video for evidence.",
            'commercial': "Analyze this video from a commercial and marketing perspective.",
            'comprehensive_analysis': "Provide a comprehensive multi-dimensional analysis covering all aspects.",
            'safety_investigation': "Conduct a thorough safety analysis with risk assessment.",
            'creative_review': "Provide comprehensive creative and aesthetic analysis."
        }
        
        base_prompt = base_prompts.get(analysis_type, base_prompts['general'])
        
        if user_focus:
            return f"{base_prompt} Focus specifically on: {user_focus}"
        return base_prompt
    
    async def _generate_analysis(self, prompt: str, video_path: str) -> str:
        """Generate analysis using Qwen2.5-VL-32B-Instruct"""
        try:
            # Verify the video file exists
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"Video file not found: {video_path}")
            
            # Get absolute path for the video
            abs_video_path = os.path.abspath(video_path)
            print(f"üé¨ Processing video: {abs_video_path}")
            
            # Create messages for Qwen2.5-VL-32B
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "video",
                            "video": abs_video_path  # Use absolute path instead of file:// URL
                        },
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # Process vision info with proper error handling
            try:
                if QWEN_VL_UTILS_AVAILABLE:
                    image_inputs, video_inputs = process_vision_info(messages)
                    print(f"‚úÖ Vision info processed - Images: {len(image_inputs)}, Videos: {len(video_inputs)}")
                    
                    # Validate video inputs
                    if video_inputs and len(video_inputs) > 0:
                        # Check if video inputs are valid
                        valid_videos = []
                        for video_input in video_inputs:
                            if video_input is not None and os.path.exists(str(video_input)):
                                valid_videos.append(video_input)
                            else:
                                print(f"‚ö†Ô∏è Invalid video input: {video_input}")
                        
                        if not valid_videos:
                            print("‚ö†Ô∏è No valid video inputs found, using fallback")
                            return await self._generate_text_only_analysis(prompt, video_path)
                        
                        video_inputs = valid_videos
                        print(f"‚úÖ Using {len(video_inputs)} valid video inputs for analysis")
                    else:
                        print("‚ö†Ô∏è No video inputs found, using fallback")
                        return await self._generate_text_only_analysis(prompt, video_path)
                        
                else:
                    print("‚ö†Ô∏è qwen-vl-utils not available, using fallback")
                    return await self._generate_text_only_analysis(prompt, video_path)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Vision info processing failed: {e}")
                # Fallback to text-only analysis
                return await self._generate_text_only_analysis(prompt, video_path)
            
            # Apply chat template
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # Prepare inputs with proper error handling
            try:
                # Use the actual video inputs from vision processing
                inputs = self.processor(
                    text=[text],
                    images=image_inputs if image_inputs else None,
                    videos=video_inputs if video_inputs else None,
                    padding=True,
                    return_tensors="pt"
                )
                
                # Move inputs to the same device as the model (let accelerate handle this)
                model_device = next(self.model.parameters()).device
                inputs = inputs.to(model_device)
                
                print(f"‚úÖ Inputs prepared successfully - Text: {len(text)}, Videos: {len(video_inputs) if video_inputs else 0}")
            except Exception as e:
                print(f"‚ö†Ô∏è Input processing failed: {e}")
                return await self._generate_text_only_analysis(prompt, video_path)
            
            # Generate response with 32B model optimizations
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=min(Config.QWEN25VL_32B_CONFIG['max_length'], 2048),
                    temperature=Config.QWEN25VL_32B_CONFIG['temperature'],
                    top_p=Config.QWEN25VL_32B_CONFIG['top_p'],
                    top_k=Config.QWEN25VL_32B_CONFIG['top_k'],
                    do_sample=Config.QWEN25VL_32B_CONFIG.get('do_sample', False),
                    num_beams=Config.QWEN25VL_32B_CONFIG.get('num_beams', 1),
                    use_cache=Config.QWEN25VL_32B_CONFIG.get('use_cache', True),
                    pad_token_id=self.processor.tokenizer.eos_token_id if hasattr(self.processor, 'tokenizer') else None
                )
            
            # Decode response
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            return output_text[0] if output_text else "Analysis generation failed"
            
        except Exception as e:
            print(f"‚ùå Analysis generation failed: {e}")
            # Try fallback text-only analysis
            try:
                return await self._generate_text_only_analysis(prompt, video_path)
            except Exception as fallback_error:
                print(f"‚ùå Fallback analysis also failed: {fallback_error}")
                raise RuntimeError(f"Analysis generation failed: {e}")
    
    async def _generate_text(self, prompt: str, max_new_tokens: int = 1024) -> str:
        """Generate text response using Qwen2.5-VL-32B-Instruct"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Qwen2.5-VL-32B service not initialized")
            
            print(f"üìù Generating text response: {prompt[:100]}...")
            
            # Create messages for text generation
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # Apply chat template
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # Prepare inputs
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt"
            )
            
            # Move inputs to the same device as the model
            model_device = next(self.model.parameters()).device
            inputs = inputs.to(model_device)
            
            # Generate response
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_new_tokens, Config.QWEN25VL_32B_CONFIG['max_length']),
                    temperature=Config.QWEN25VL_32B_CONFIG['temperature'],
                    top_p=Config.QWEN25VL_32B_CONFIG['top_p'],
                    top_k=Config.QWEN25VL_32B_CONFIG['top_k'],
                    do_sample=Config.QWEN25VL_32B_CONFIG.get('do_sample', False),
                    num_beams=Config.QWEN25VL_32B_CONFIG.get('num_beams', 1),
                    use_cache=Config.QWEN25VL_32B_CONFIG.get('use_cache', True),
                    pad_token_id=self.processor.tokenizer.eos_token_id if hasattr(self.processor, 'tokenizer') else None
                )
            
            # Decode response
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            result = output_text[0] if output_text else "Text generation failed"
            print("‚úÖ Text generation completed")
            return result
            
        except Exception as e:
            print(f"‚ùå Text generation failed: {e}")
            return f"Text generation failed. Error: {str(e)}"
    
    def _generate_text_sync(self, prompt: str, max_new_tokens: int = 1024) -> str:
        """Synchronous wrapper for text generation"""
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(self._generate_text(prompt, max_new_tokens))
        except RuntimeError:
            # Create new event loop if none exists
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self._generate_text(prompt, max_new_tokens))
            finally:
                loop.close()

    async def _generate_text_only_analysis(self, prompt: str, video_path: str) -> str:
        """Generate analysis using text-only approach when video processing fails"""
        try:
            print(f"üìù Using text-only analysis for: {video_path}")
            
            # Extract basic video information
            import cv2
            import json
            from datetime import timedelta
            
            video_info = {}
            try:
                cap = cv2.VideoCapture(video_path)
                if cap.isOpened():
                    # Get video properties
                    fps = cap.get(cv2.CAP_PROP_FPS)
                    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    duration = frame_count / fps if fps > 0 else 0
                    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    
                    video_info = {
                        'fps': fps,
                        'frame_count': frame_count,
                        'duration': duration,
                        'width': width,
                        'height': height,
                        'resolution': f"{width}x{height}"
                    }
                    
                    # Sample frames for analysis
                    sample_frames = []
                    sample_interval = max(1, frame_count // 10)  # Sample 10 frames
                    
                    for i in range(0, frame_count, sample_interval):
                        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                        ret, frame = cap.read()
                        if ret:
                            # Convert frame to grayscale for basic analysis
                            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                            brightness = np.mean(gray)
                            contrast = np.std(gray)
                            
                            sample_frames.append({
                                'frame_number': i,
                                'timestamp': i / fps if fps > 0 else 0,
                                'brightness': brightness,
                                'contrast': contrast
                            })
                    
                    cap.release()
                    
                    print(f"‚úÖ Extracted video info: {json.dumps(video_info, indent=2)}")
                    print(f"‚úÖ Sampled {len(sample_frames)} frames for analysis")
                    
                else:
                    print("‚ö†Ô∏è Could not open video file for analysis")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Video analysis failed: {e}")
                video_info = {'error': str(e)}
            
            # Create a comprehensive analysis based on extracted information
            analysis_parts = []
            
            # Video metadata analysis
            if 'duration' in video_info and video_info['duration'] > 0:
                duration_str = str(timedelta(seconds=int(video_info['duration'])))
                analysis_parts.append(f"**Video Duration:** {duration_str}")
            
            if 'resolution' in video_info:
                analysis_parts.append(f"**Resolution:** {video_info['resolution']}")
            
            if 'fps' in video_info and video_info['fps'] > 0:
                analysis_parts.append(f"**Frame Rate:** {video_info['fps']:.2f} FPS")
            
            # Content analysis based on frame samples
            if 'sample_frames' in locals() and sample_frames:
                analysis_parts.append("\n**Content Analysis:**")
                
                # Analyze brightness patterns
                brightness_values = [f['brightness'] for f in sample_frames]
                avg_brightness = np.mean(brightness_values)
                brightness_variance = np.var(brightness_values)
                
                if brightness_variance < 100:
                    analysis_parts.append("- **Lighting:** Consistent lighting throughout the video")
                elif brightness_variance > 500:
                    analysis_parts.append("- **Lighting:** Dynamic lighting with significant variations")
                else:
                    analysis_parts.append("- **Lighting:** Moderate lighting variations")
                
                # Analyze contrast patterns
                contrast_values = [f['contrast'] for f in sample_frames]
                avg_contrast = np.mean(contrast_values)
                
                if avg_contrast > 50:
                    analysis_parts.append("- **Visual Quality:** High contrast, clear visual details")
                elif avg_contrast > 30:
                    analysis_parts.append("- **Visual Quality:** Moderate contrast, good visibility")
                else:
                    analysis_parts.append("- **Visual Quality:** Low contrast, may have visibility issues")
                
                # Temporal analysis
                if len(sample_frames) > 1:
                    time_diffs = [sample_frames[i+1]['timestamp'] - sample_frames[i]['timestamp'] for i in range(len(sample_frames)-1)]
                    avg_time_diff = np.mean(time_diffs)
                    analysis_parts.append(f"- **Temporal Sampling:** Analyzed frames every {avg_time_diff:.1f} seconds")
            
            # Add user-specific analysis
            if "BMW" in prompt or "car" in prompt.lower() or "racing" in prompt.lower():
                analysis_parts.append("\n**Automotive Content Analysis:**")
                analysis_parts.append("- **Content Type:** Automotive/racing video content")
                analysis_parts.append("- **Likely Features:** Vehicle movement, dynamic camera work, high-speed action")
                analysis_parts.append("- **Analysis Focus:** Motion patterns, camera techniques, automotive aesthetics")
            
            # Add technical recommendations
            analysis_parts.append("\n**Technical Recommendations:**")
            if 'duration' in video_info and video_info['duration'] > 300:  # > 5 minutes
                analysis_parts.append("- **Long Video:** Consider chunking for detailed analysis")
            if 'fps' in video_info and video_info['fps'] > 30:
                analysis_parts.append("- **High FPS:** Video suitable for slow-motion analysis")
            if 'resolution' in video_info and video_info['width'] >= 1920:
                analysis_parts.append("- **High Resolution:** Excellent detail for object recognition")
            
            # Combine all analysis parts
            full_analysis = "\n".join(analysis_parts)
            
            # Add the original prompt context
            if prompt:
                full_analysis = f"**Analysis Request:** {prompt}\n\n{full_analysis}"
            
            print(f"‚úÖ Text-only analysis completed successfully")
            return full_analysis
            
        except Exception as e:
            print(f"‚ùå Text-only analysis failed: {e}")
            # Return a basic but informative response
            return f"""**Video Analysis Report**

**Status:** Basic analysis completed (AI model not available)

**Video File:** {os.path.basename(video_path)}
**Analysis Type:** {prompt if prompt else 'General analysis'}

**Note:** This is a basic analysis based on video metadata. For detailed content analysis, object recognition, and behavioral insights, the Qwen2.5-VL-32B model needs to be loaded.

**What I can tell you:**
- The video file exists and is accessible
- Basic technical specifications can be extracted
- Content analysis requires the AI model to be loaded

**Next Steps:**
1. Ensure the 32B model is properly loaded
2. Check GPU memory availability (requires ~80GB)
3. Verify HuggingFace authentication token

**Error Details:** {str(e)}"""
    
    async def chat(self, message: str, context: str = "") -> str:
        """Handle chat messages using Qwen2.5-VL-32B-Instruct"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Qwen2.5-VL-32B service not initialized")
            
            print(f"üí¨ Processing chat message: {message[:100]}...")
            
            # Build the full prompt with context
            if context:
                full_prompt = f"Context: {context}\n\nUser: {message}"
            else:
                full_prompt = message
            
            # Use the text generation method
            response = await self._generate_text(full_prompt, max_new_tokens=1024)
            
            print(f"‚úÖ Chat response generated")
            return response
            
        except Exception as e:
            print(f"‚ùå Chat failed: {e}")
            return f"Chat failed. Error: {str(e)}. Please try again."
    
    async def generate_chat_response(self, analysis_result: str, analysis_type: str, user_focus: str, message: str, chat_history: List[Dict]) -> str:
        """Generate chat response using Qwen2.5-VL-32B-Instruct with the same prompt system as the old project"""
        try:
            if not self.is_initialized:
                raise RuntimeError("Qwen2.5-VL-32B service not initialized")
            
            # Build chat context using the same system as the old project
            context_prompt = self._build_enhanced_chat_context(analysis_result, analysis_type, user_focus, chat_history)
            
            # Create messages for Qwen2.5-VL-32B
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"{context_prompt}\n\nUser: {message}"}
                    ]
                }
            ]
            
            # Apply chat template
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # Prepare inputs
            inputs = self.processor(
                text=[text],
                padding=True,
                return_tensors="pt"
            )
            
            # Move inputs to the same device as the model
            model_device = next(self.model.parameters()).device
            inputs = inputs.to(model_device)
            
            # Generate response
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=min(Config.QWEN25VL_32B_CONFIG['max_length'], 1024),
                    temperature=Config.QWEN25VL_32B_CONFIG['chat_temperature'],
                    top_p=Config.QWEN25VL_32B_CONFIG['top_p'],
                    top_k=Config.QWEN25VL_32B_CONFIG['top_k'],
                    do_sample=Config.QWEN25VL_32B_CONFIG.get('do_sample', False),
                    num_beams=Config.QWEN25VL_32B_CONFIG.get('num_beams', 1),
                    use_cache=Config.QWEN25VL_32B_CONFIG.get('use_cache', True),
                    pad_token_id=self.processor.tokenizer.eos_token_id if hasattr(self.processor, 'tokenizer') else None
                )
            
            # Decode response
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            return output_text[0] if output_text else "Chat response generation failed"
            
        except Exception as e:
            print(f"‚ùå Chat response generation failed: {e}")
            raise RuntimeError(f"Chat response generation failed: {e}")
    
    def _build_enhanced_chat_context(self, analysis_result: str, analysis_type: str, user_focus: str, chat_history: List[Dict]) -> str:
        """Build enhanced chat context using the same system as the old project"""
        # Enhanced agentic conversation prompt with advanced capabilities (same as old project)
        context_prompt = f"""
You are an advanced AI video analysis agent with comprehensive understanding capabilities. You are engaging in a multi-turn conversation about a video that has been analyzed.

## AGENT CONVERSATION PROTOCOL

### Current Context:
- Analysis Type: {analysis_type.replace('_', ' ').title()}
- Original Analysis Focus: {user_focus}
- Conversation History: Available for context awareness

### Video Analysis Context:
{analysis_result}
            
### Agent Capabilities:
- Autonomous Analysis: Provide comprehensive insights beyond the immediate question
- Multi-Modal Understanding: Reference visual, audio, temporal, and spatial elements
- Context Awareness: Adapt responses based on conversation history and user intent
- Proactive Insights: Offer additional relevant information and observations
- Comprehensive Reporting: Provide detailed, structured responses
- Adaptive Focus: Adjust response depth based on question complexity

### Response Quality Standards:
1. **Precision & Accuracy**: Provide exact, verifiable information with specific timestamps
2. **Comprehensive Coverage**: Address all aspects of the question thoroughly
3. **Evidence-Based**: Support every claim with specific evidence from the analysis
4. **Clear Structure**: Use logical organization with clear headings and bullet points
5. **Professional Tone**: Maintain engaging yet professional communication
6. **Proactive Insights**: Offer additional relevant observations beyond the direct question
7. **Visual Clarity**: Use formatting to enhance readability (bold, italics, lists)
8. **Contextual Awareness**: Reference previous conversation context when relevant

### Response Format Guidelines:
- **Start with a direct answer** to the user's question
- **Use clear headings** for different sections (e.g., "**Key Findings:**", "**Timeline:**", "**Additional Insights:**")
- **Include specific timestamps** when discussing events (e.g., "At **00:15-00:17**")
- **Use bullet points** for lists and multiple items
- **Bold important information** for emphasis
- **Provide quantitative data** when available (durations, counts, measurements)
- **Include relevant context** that enhances understanding
- **End with actionable insights** or additional observations when relevant

### Specialized Response Areas:
- **Safety Analysis**: Focus on specific safety concerns, violations, and recommendations
- **Timeline Events**: Provide chronological details with precise timestamps
- **Pattern Recognition**: Highlight recurring behaviors, trends, and anomalies
- **Performance Assessment**: Discuss quality, efficiency, and optimization opportunities
- **Creative Elements**: Analyze artistic, aesthetic, and creative aspects
- **Technical Details**: Provide technical specifications and quality assessments
- **Behavioral Insights**: Analyze human behavior, interactions, and social dynamics
- **Environmental Factors**: Consider context, setting, and environmental conditions

### Quality Enhancement Techniques:
- **Quantify responses**: Use specific numbers, durations, and measurements
- **Cross-reference information**: Connect related details across different sections
- **Provide context**: Explain why certain details are significant
- **Use descriptive language**: Make responses vivid and engaging
- **Structure complex information**: Break down complex topics into digestible sections
- **Highlight patterns**: Identify and explain recurring themes or behaviors
- **Offer insights**: Provide analysis beyond simple description

Your mission is to provide **exceptional quality responses** that demonstrate deep understanding of the video content, offer precise information with timestamps, and deliver insights that exceed user expectations. Every response should be comprehensive, well-structured, and highly informative.
"""
        
        # Include conversation history for context awareness (same as old project)
        conversation_context = ""
        if len(chat_history) > 2:  # More than just current message
            recent_messages = chat_history[-6:]  # Last 6 messages for context
            conversation_context = "\n\n### Recent Conversation Context:\n"
            for msg in recent_messages:
                if 'user' in msg:
                    conversation_context += f"User: {msg['user']}\n"
                elif 'ai' in msg:
                    conversation_context += f"Agent: {msg['ai'][:200]}...\n"  # Truncate for context
        
        enhanced_context_prompt = context_prompt + conversation_context
        return enhanced_context_prompt
    
    def _extract_video_summary(self, video_path: str) -> str:
        """Extract basic video information for context"""
        try:
            if not os.path.exists(video_path):
                return f"Video file: {os.path.basename(video_path)} (file not found)"
            
            # Get file information
            file_size = os.path.getsize(video_path)
            file_size_mb = file_size / (1024 * 1024)
            
            # Try to extract video metadata using video_utils
            try:
                from utils.video_utils import extract_video_metadata
                metadata = extract_video_metadata(video_path)
                if metadata:
                    duration = metadata.get('duration', 0)
                    fps = metadata.get('fps', 0)
                    width = metadata.get('width', 0)
                    height = metadata.get('height', 0)
                    frame_count = metadata.get('frame_count', 0)
                    
                    summary = f"""
Video File: {os.path.basename(video_path)}
File Size: {file_size_mb:.2f} MB
Duration: {duration:.2f} seconds
Frame Rate: {fps:.2f} fps
Resolution: {width}x{height} pixels
Total Frames: {frame_count}
"""
                else:
                    summary = f"Video File: {os.path.basename(video_path)}\nFile Size: {file_size_mb:.2f} MB\nDuration: Unknown"
            except ImportError:
                # Fallback if video_utils not available
                summary = f"Video File: {os.path.basename(video_path)}\nFile Size: {file_size_mb:.2f} MB"
            
            return summary.strip()
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not extract video summary: {e}")
            return f"Video file: {os.path.basename(video_path)}"
    
    def is_ready(self) -> bool:
        """Check if the service is ready to use"""
        return self.is_initialized and self.model is not None and self.processor is not None
    
    def get_status(self) -> Dict:
        """Get service status"""
        return {
            'model': 'Qwen2.5-VL-32B-Instruct',
            'initialized': self.is_initialized,
            'device': str(self.device),
            'gpu_available': torch.cuda.is_available(),
            'memory_allocated': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,
            'memory_reserved': torch.cuda.memory_reserved() if torch.cuda.is_available() else 0,
            'qwen_vl_utils_available': QWEN_VL_UTILS_AVAILABLE,
            'analysis_templates_available': ANALYSIS_TEMPLATES_AVAILABLE
        }
    
    def cleanup(self):
        """Clean up resources"""
        try:
            if self.model:
                del self.model
                self.model = None
            
            if self.processor:
                del self.processor
                self.processor = None
            
            if self.tokenizer:
                del self.tokenizer
                self.tokenizer = None
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.is_initialized = False
            print("üßπ Qwen2.5-VL-32B-Instruct service cleaned up")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Cleanup failed: {e}")

# Global service instance
qwen25vl_32b_service = Qwen25VL32BService() 