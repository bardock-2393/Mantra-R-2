# ðŸš€ AI Video Detective Round 2 - GPU-Powered Local AI
# Requirements for high-performance, GPU-optimized video analysis
# Target: <1000ms latency, 90fps processing, 120-minute video support

# =============================================================================
# CORE GPU ACCELERATION
# =============================================================================

# PyTorch with CUDA support (GPU acceleration)
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# CUDA optimization libraries
nvidia-cuda-runtime-cu12>=12.0.0
nvidia-cudnn-cu12>=8.9.0
nvidia-tensorrt>=8.6.0

# =============================================================================
# AI MODEL FRAMEWORKS
# =============================================================================

# Transformers for MiniCPM-V 2.6 and Qwen2.5-VL
transformers>=4.40.0  # Updated for Qwen2.5-VL-32B support
accelerate>=0.20.0
bitsandbytes>=0.41.0
sentencepiece>=0.1.99
protobuf>=3.20.0

# Note: For Qwen2.5-VL-32B, build transformers from source:
# pip install git+https://github.com/huggingface/transformers accelerate

# Vector search and embeddings
sentence-transformers>=2.2.0

# Qwen2.5-VL utilities
qwen-vl-utils[decord]==0.0.8

# Flash Attention for speed optimization
flash-attn>=2.3.0
xformers>=0.0.22

# =============================================================================
# VIDEO PROCESSING & DEEPSTREAM
# =============================================================================

# OpenCV for video processing
opencv-python-headless>=4.8.0
opencv-contrib-python>=4.8.0

# High-performance video libraries
av>=10.0.0
decord>=0.6.0
ffmpeg-python>=0.2.0

# DeepStream integration
pyds>=1.1.0
nvidia-deepstream>=6.2.0

# YOLO object detection
ultralytics>=8.0.0
onnx>=1.14.0
onnxruntime-gpu>=1.15.0

# =============================================================================
# GPU MONITORING & OPTIMIZATION
# =============================================================================

# NVIDIA GPU monitoring
pynvml>=11.5.0
nvidia-ml-py3>=7.352.0

# GPU memory optimization
pytorch-memlab>=0.2.4
gputil>=1.4.0

# CUDA performance profiling
pycuda>=2022.2.2
numba>=0.57.0

# =============================================================================
# HIGH-PERFORMANCE API FRAMEWORKS
# =============================================================================

# FastAPI for high-performance API
fastapi>=0.110.0
uvicorn[standard]>=0.27.0

# WebSocket support for real-time streaming
websockets>=12.0.0
python-socketio>=5.8.0

# ASGI server optimization
gunicorn>=21.2.0
hypercorn>=0.15.0

# =============================================================================
# MEMORY & PERFORMANCE OPTIMIZATION
# =============================================================================

# Memory management
psutil>=5.9.0
memory-profiler>=0.61.0

# Performance profiling
py-spy>=0.3.14
pyinstrument>=4.6.0

# Async optimization
asyncio-mqtt>=0.13.0
aiofiles>=23.2.0

# =============================================================================
# DATA PROCESSING & STORAGE
# =============================================================================

# Efficient data handling
numpy>=1.24.0
pandas>=2.0.0
pillow>=10.0.0

# Video metadata extraction
python-ffmpeg>=2.0.4
moviepy>=1.0.3

# File handling optimization
pathlib2>=2.3.7
watchdog>=3.0.0

# =============================================================================
# SESSION & CACHE MANAGEMENT
# =============================================================================

# Local file-based sessions
pickle5>=0.0.12
dill>=0.3.7

# In-memory caching
cachetools>=5.3.0
diskcache>=5.6.0

# =============================================================================
# SECURITY & VALIDATION
# =============================================================================

# Input validation
pydantic>=2.0.0
marshmallow>=3.20.0

# Security headers
secure>=0.3.0
python-multipart>=0.0.6

# =============================================================================
# DEVELOPMENT & TESTING
# =============================================================================

# Testing framework
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-benchmark>=4.0.0

# Code quality
black>=23.0.0
flake8>=6.0.0
mypy>=1.5.0

# =============================================================================
# LOGGING & MONITORING
# =============================================================================

# Structured logging
structlog>=23.1.0
loguru>=0.7.0

# Performance monitoring
prometheus-client>=0.17.0
statsd>=4.0.1

# =============================================================================
# DEPLOYMENT & CONTAINERIZATION
# =============================================================================

# Container support
docker>=6.1.0
kubernetes>=26.1.0

# Process management
supervisor>=4.2.5
circus>=0.18.0

# =============================================================================
# OPTIONAL ENHANCEMENTS
# =============================================================================

# Multi-GPU support
torch-elastic>=0.2.0
horovod>=0.27.0

# Advanced video analytics
opencv-python-contrib>=4.8.0
mediapipe

# Cloud integration (optional)
boto3>=1.28.0
google-cloud-storage>=2.10.0

# =============================================================================
# SYSTEM REQUIREMENTS
# =============================================================================

# Minimum system requirements:
# - NVIDIA GPU with CUDA 12.0+ support
# - 8GB+ GPU VRAM (80GB recommended)
# - 16GB+ system RAM
# - Python 3.9+
# - Ubuntu 20.04+ or Windows 10+

# =============================================================================
# INSTALLATION NOTES
# =============================================================================

# For GPU support, install PyTorch with CUDA:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# For DeepStream support:
# Follow NVIDIA DeepStream installation guide for your platform

# For optimal performance:
# - Use CUDA 12.1+ for latest optimizations
# - Enable TensorRT for YOLO models
# - Configure GPU memory allocation in config.py 